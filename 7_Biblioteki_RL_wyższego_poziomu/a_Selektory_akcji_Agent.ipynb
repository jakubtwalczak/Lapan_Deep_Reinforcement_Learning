{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT+ZZDyot2xARnt3WHzQtW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Selektory akcji"
      ],
      "metadata": {
        "id": "EDfW0gt_RZyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "DComhDzbCm7D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvDDfrNv9Ply",
        "outputId": "edbec936-70f9-4693-96a5-ed787cb7a25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
          ]
        }
      ],
      "source": [
        "import ptan\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_vals = np.array([[1, 2, 3], [1, -1, 0]])\n",
        "q_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8eEKSal9TSY",
        "outputId": "687509de-e77c-4905-f4ce-b307103e13fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 1, -1,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector = ptan.actions.ArgmaxActionSelector()\n",
        "print(\"argmax:\", selector(q_vals))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "772IpCdACgPM",
        "outputId": "855b3eaa-430f-4be1-c851-ee6002c25654"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax: [2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0)\n",
        "print(\"epsilon=0.0:\", selector(q_vals))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A-N6K91C5nc",
        "outputId": "d9d363a4-b56c-4eee-f745-6b79cd58f17f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epsilon=0.0: [2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector.epsilon = 0.5\n",
        "print(\"epsilon=0.5:\", selector(q_vals))\n",
        "selector.epsilon = 0.1\n",
        "print(\"epsilon=0.1:\", selector(q_vals))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz6nifMjDRML",
        "outputId": "06e58e60-68f3-425c-f15f-52586485e9a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epsilon=0.5: [2 1]\n",
            "epsilon=0.1: [2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector = ptan.actions.ProbabilityActionSelector()\n",
        "print(\"Akcje wybrane z trzech rozkładów prawdopodobieństwa:\")\n",
        "for _ in range(10):\n",
        "    acts = selector(np.array([\n",
        "          [0.1, 0.8, 0.1],\n",
        "          [0.0, 0.0, 1.0],\n",
        "          [0.5, 0.5, 0.0]\n",
        "        ]))\n",
        "    print(acts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzYC-n3VDUhq",
        "outputId": "c9d737ba-6317-43e6-de92-042b2b51c96a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akcje wybrane z trzech rozkładów prawdopodobieństwa:\n",
            "[0 2 0]\n",
            "[1 2 1]\n",
            "[0 2 1]\n",
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[1 2 1]\n",
            "[0 2 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "ctWUsc03Rdd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DQNAgent\n",
        "# stosowany, gdy przestrzeń akcji nie jest zbyt duża (np. gry Atari)\n",
        "# dane wejściowe - paczka obserwacji w formie tablicy NumPy\n",
        "# obserwacje przekazywane są do sieci w celu uzyskania wartości Q\n",
        "# następnie stosowany jest ActionSelector do konwersji tych wartości na indeksy odpowiadające akcjom\n",
        "\n",
        "class DQNNet(nn.Module):\n",
        "    def __init__(self, actions: int):\n",
        "        super(DQNNet, self).__init__()\n",
        "        self.actions = actions\n",
        "\n",
        "    def forward(self, x):\n",
        "\t\t  # zawsze tworzymy diagonalny tensor o kształcie (rozmiar_paczki, akcje)\n",
        "        return torch.eye(x.size()[0], self.actions)"
      ],
      "metadata": {
        "id": "wmZWVRVbRerk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = DQNNet(actions=3) # używamy agenta jako modelu głębokiej sieci Q\n",
        "net_out = net(torch.zeros(2, 10))\n",
        "print(\"dqn_net:\")\n",
        "print(net_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Uc833VSskb",
        "outputId": "692335db-e16b-44b2-a9f6-b9f9696d44d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dqn_net:\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector = ptan.actions.ArgmaxActionSelector() # polityka argmax - agent zwróci akcje odpowiadające wartościom 1.0 w danych wyjściowych sieci\n",
        "agent = ptan.agent.DQNAgent(model=net, action_selector=selector)\n",
        "ag_out = agent(torch.zeros(2, 5)) # dane wejściowe - dwie obserwacje po pięć wartości\n",
        "print(\"Argmax:\", ag_out) # agent zwraca krotkę z dwoma obiektami:\n",
        "# 1. tablica z akcjami do wykonania dla obu obserwacji\n",
        "# 2. lista ze stanem wewnętrznym agenta (obecny agent bezstanowy, lista zawiera wartości None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxQhS5xOTCJj",
        "outputId": "9db0d128-bd07-4f61-97a1-518a9bd3d0e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Argmax: (array([0, 1]), [None, None])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0) # polityka epsilonu zachłannego\n",
        "#wszystkie akcje będą losowe, niezależnie od danych wyjściowych sieci\n",
        "agent = ptan.agent.DQNAgent(model=net, action_selector=selector)\n",
        "ag_out = agent(torch.zeros(10, 5))[0]\n",
        "print(\"eps=1.0:\", ag_out)\n",
        "\n",
        "selector.epsilon = 0.5 # możemy \"w locie\" zmniejszać tę wartość\n",
        "# jest to przydatne podczas treningu i stopniowego zmniejszania epsilonu\n",
        "ag_out = agent(torch.zeros(10, 5))[0]\n",
        "print(\"eps=0.5:\", ag_out)\n",
        "\n",
        "selector.epsilon = 0.1\n",
        "ag_out = agent(torch.zeros(10, 5))[0]\n",
        "print(\"eps=0.1:\", ag_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cblz86YTFay",
        "outputId": "985ea04f-0d42-4a13-8323-6c69f9a73bba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eps=1.0: [0 0 0 2 2 2 0 1 0 0]\n",
            "eps=0.5: [0 1 2 0 0 0 2 2 0 0]\n",
            "eps=0.1: [0 1 2 0 0 1 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PolicyNet\n",
        "# obiekt tej klasy oczekuje generowania przez sieć polityki w postaci dyskretnego zestawu akcji\n",
        "# dystrybucja ta może być nieznormalizowana (realizowana funkcją logitową) lub znormalizowana\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, actions: int):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.actions = actions\n",
        "\n",
        "    def forward(self, x):\n",
        "\t\t  # Teraz tworzymy tensor z dwiema pierwszymi akcjami\n",
        "\t\t  # o tych samych wartościach funkcji logitowej\n",
        "        shape = (x.size()[0], self.actions)\n",
        "        res = torch.zeros(shape, dtype=torch.float32)\n",
        "        res[:, 0] = 1\n",
        "        res[:, 1] = 1\n",
        "        return res # sieć generuje wartości prawdopodobieństw"
      ],
      "metadata": {
        "id": "sMW3FWDXR_mu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = PolicyNet(actions=5)\n",
        "net_out = net(torch.zeros(6, 10))\n",
        "print(\"policy_net:\")\n",
        "print(net_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nnt7Endann7",
        "outputId": "9f02b273-70bd-41fa-e194-c0c9c148f9b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_net:\n",
            "tensor([[1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selector = ptan.actions.ProbabilityActionSelector()\n",
        "agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True)\n",
        "# klasa PolicyAgent powinna zastosować do danych wyjściowych sieci funkcję softmax\n",
        "ag_out = agent(torch.zeros(6, 5))[0]\n",
        "print(ag_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn3fXRidaoGP",
        "outputId": "e0cffc9c-dff5-4542-9369-cd8cc901cca5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.functional.softmax(net(torch.zeros(1, 10)), dim=1)\n",
        "# funkcja softmax zwróci niezerowe prawdopodobieństwa, również dla logitów = 0\n",
        "# agent może więc wybrać więcej niż jedną akcję"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh0W0d9EdHzP",
        "outputId": "7bf39d38-70ed-4513-be1b-7c1034f27458"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3222, 0.3222, 0.1185, 0.1185, 0.1185]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}