{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBDYuKUaM60JEl7w7PiZPf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IzUa9D2VokB",
        "outputId": "b0a4b6fe-df3a-4bed-d1e0-7c3d57e05d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "import gym\n",
        "import ptan\n",
        "from typing import List, Optional, Tuple, Any"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Środowisko z obserwacjami o wartościach od 0 do 4 i akcjami od 0 do 2.\n",
        "    Obserwacje zmieniają się cyklicznie zgodnie z operacją modulo 5,\n",
        "\t  a nagroda jest równa wartości akcji.\n",
        "    Epizody mają stałą długość równą 10.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ToyEnv, self).__init__()\n",
        "        self.observation_space = gym.spaces.Discrete(n=5)\n",
        "        self.action_space = gym.spaces.Discrete(n=3)\n",
        "        self.step_index = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_index = 0\n",
        "        return self.step_index, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        is_done = self.step_index == 10\n",
        "        if is_done:\n",
        "            # Dodanie is_truncated (zawsze False w tym przypadku)\n",
        "            return self.step_index % self.observation_space.n, \\\n",
        "                   0.0, is_done, False, {}\n",
        "        self.step_index += 1\n",
        "        # Dodanie is_truncated (zawsze False w tym przypadku)\n",
        "        return self.step_index % self.observation_space.n, \\\n",
        "               float(action), self.step_index == 10, False, {}\n",
        "\n",
        "\n",
        "class DullAgent(ptan.agent.BaseAgent):\n",
        "    \"\"\"\n",
        "    Agent zawsze zwraca taką samą akcję.\n",
        "    \"\"\"\n",
        "    def __init__(self, action: int):\n",
        "        self.action = action\n",
        "\n",
        "    def __call__(self, observations: List[Any],\n",
        "                 state: Optional[List] = None) \\\n",
        "            -> Tuple[List[int], Optional[List]]:\n",
        "        return [self.action for _ in observations], state"
      ],
      "metadata": {
        "id": "vsZaysw5aQd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e6bcae-c3a9-4202-f9a9-5987360e31d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Klasa ExperienceSource\n",
        "# generuje fragmenty trajektorii utworzonych przez akcje agenta w środowisku z wszystkimi krokami pośrednimi\n",
        "# wykorzystuje kilka argumentów: środowisko, instancję agenta, liczbę kroków\n",
        "# opcjonalny jest parametr vectorized (domyślnie False)\n",
        "# jeżeli True, środowisko musi być wektoryzowanym środowiskiem OpenAI Universe\n",
        "\n",
        "env = ToyEnv()\n",
        "agent = DullAgent(action=1)\n",
        "exp_source = ptan.experience.ExperienceSource(env, agent, steps_count=2)\n",
        "for idx, exp in enumerate(exp_source): # każda iteracja zwraca fragment trajektorii agenta\n",
        "# w postaci krotek o długości równej parametrowi step_count lub mniejszej\n",
        "    if idx > 2:\n",
        "        break\n",
        "    print(exp)\n",
        "\n",
        "# Co dzieje się w trakcie iteracji:\n",
        "# 1. wywoływana jest funkcja reset() dla środowiska - uzyskanie stanu początkowego\n",
        "# 2. agent wybiera akcję na podstawie zwróconego stanu\n",
        "# 3. wywoływana jest metoda step() dla uzyskania nagrody i następnego stanu\n",
        "# 4. następny stan jest przekazywany agentowi w celu wykonania kolejnej akcji\n",
        "# 5. zwrócona zostaje informacja o przejściu z jednego stanu do drugiego\n",
        "# 6. kroki 3. - 5. zostają powtórzone, aż do przetworzenia źródła doświadczenia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aveIZTiNgDlt",
        "outputId": "ba5bbd81-a07a-4f45-cedd-4714a5ce91ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Każdy obiekt w krotce zwracanej przez ExperienceSource jest instancją klasy ptan.experience.Experience\n",
        "# ta klasa to kontener namedtuple z polami:\n",
        "# 1. state - stan obserwowany przed podjęciem akcji\n",
        "# 2. action - wykonana akcja\n",
        "# 3. reward - natychmiastowa nagroda otrzymana ze środowiska\n",
        "# 4. done_trunc - flaga oznaczająca zakończenie epizodu\n",
        "\n",
        "# jeżeli epizod dobiegnie końca, fragment trajektorii będzie krótszy, a środowisko zostanie automatycznie zresetowane\n",
        "\n",
        "for idx, exp in enumerate(exp_source):\n",
        "    if idx > 15:\n",
        "        break\n",
        "    print(exp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fvMWQW3gVG8",
        "outputId": "fc170548-8c83-44ef-c0d5-9ff3209fff28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=4, action=1, reward=1.0, done_trunc=False), Experience(state=0, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=True))\n",
            "(Experience(state=4, action=1, reward=1.0, done_trunc=True),)\n",
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=4, action=1, reward=1.0, done_trunc=False), Experience(state=0, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# możemy zażądać od klasy ExperienceSource wygenerowania trajektorii składowych o dowolnej długości\n",
        "\n",
        "exp_source = ptan.experience.ExperienceSource(\n",
        "        env=env, agent=agent, steps_count=4)\n",
        "print(next(iter(exp_source)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1wQ2uCIpUhv",
        "outputId": "f087d1ae-23bb-48e1-bf42-729283245215"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obiektowi ww. klasy można przekazać kilka instancji klasy gym.Env - używane będą cyklicznie\n",
        "\n",
        "env1 = ToyEnv()\n",
        "env2 = ToyEnv()\n",
        "\n",
        "exp_source = ptan.experience.ExperienceSource(\n",
        "        env=[env1, env2], agent=agent, steps_count=4)\n",
        "for idx, exp in enumerate(exp_source):\n",
        "    if idx > 4:\n",
        "        break\n",
        "    print(exp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6618C1I4tFpJ",
        "outputId": "77ccc694-8ad6-4a1a-e966-b936fd8c4470"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False))\n",
            "(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False), Experience(state=0, action=1, reward=1.0, done_trunc=False))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# klasa ExperienceSourceFirstLast\n",
        "# zamiast pełnej trajektorii składowej zwraca jedynie krok pierwszy i krok ostatni oraz dba o właściwą akumulację nagrody\n",
        "# dziedziczy po ExperienceSource, ale zwraca inne dane - krotkę z polami:\n",
        "# 1. state - stan, w którym zdecydowano się na podjęcie akcji\n",
        "# 2. action - akcja wykonana w danym kroku\n",
        "# 3. reward - częściowo skumulowana nagroda za step_count kroków (jeżeli step_count=1, nagroda natychmiastowa)\n",
        "# 4. last_state - stan osiągnięty po wykonaniu akcji (jeżeli epizod się kończy, wartością jest None)\n",
        "\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
        "    env, agent, gamma=1.0, steps_count=1)\n",
        "for idx, exp in enumerate(exp_source):\n",
        "    if idx > 10:\n",
        "        break\n",
        "    print(exp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAmMSbYjtsT-",
        "outputId": "c43d5f93-9d42-4d34-af7a-9b2178ffe801"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
            "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
            "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
            "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
            "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
            "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
            "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dane te są lepsze w przypadku treningu głębokiej sieci Q, bo można ich użyć w przybliżeniu Bellmana bezpśrednio\n",
        "# sprawdźmy wynik w razie zastosowania większej liczby kroków\n",
        "\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
        "    env, agent, gamma=1.0, steps_count=2)\n",
        "for idx, exp in enumerate(exp_source):\n",
        "    if idx > 10:\n",
        "        break\n",
        "    print(exp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwObrFrrvFRX",
        "outputId": "0b663605-9fdb-4d39-c593-bcf69f7f28e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
            "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
            "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
            "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=0)\n",
            "ExperienceFirstLast(state=4, action=1, reward=2.0, last_state=1)\n",
            "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
            "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
            "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
            "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
            "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
            "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n"
          ]
        }
      ]
    }
  ]
}